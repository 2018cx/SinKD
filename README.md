# Sinkhorn Distance Minimization for Knowledge Distillation
## Installation
## Download GLUE Data
Download the GLUE data using this repository or from GLUE benchmark website, unpack it to directory datas/glue and rename the folder CoLA to COLA.

## Download Pre-trained BERT
Download bert_uncased_L-12_H-768_A-12 (BERT-base) and bert_uncased_L-6_H-768_A-12 for teacher model and student model, respectively, from this repository. and use the API from Huggingface to transform them to pytorch checkpoint.

## Task-specific BERT Model Distillation

## Task-specific T0 Model Distillation

## Task-specific GPT Model Distillation
